Challenge scenario
The company has to process an ever increasing mountain of documents that all require individual manual processing for validation and authorization, which is an expensive task that requires a lot of staff. The company plans to leverage GCP tools to automate the process of collecting, categorizing, and verifying documents in an efficient and less labor intensive manner.
Your challenge
you must create a document processing pipeline that will automatically process documents that are uploaded to Cloud Storage. The pipeline consists of a primary Cloud Function that processes new files that are uploaded to Cloud Storage using a Document AI form processor to extract the data from the document. The function then saves the form data detected in those files to BigQuery.
You are provided with the source code for a Cloud Function that will perform the processing, and you are expected to deploy the document processing pipeline as shown in the architecture below, making sure to correctly configure the components for your specific pipeline.



###
Activate Cloud Shell
Click Activate Cloud Shell  at the top of the Google Cloud console.
	gcloud auth list
You can list the project ID with this command:
	gcloud config list project



###
Task 1. Enable the Cloud Document AI API and copy lab source files.
In this task, you must enable the Cloud Document AI API and copy your starter files into Cloud Shell.
Enable the Cloud Document AI API
Enable the Cloud Document AI API.
Enable Cloud Document AI API

*** ???
In Cloud Shell, enter the following commands to enable the APIs required by the lab:
 gcloud services enable documentai.googleapis.com      
  gcloud services enable cloudfunctions.googleapis.com  
  gcloud services enable cloudbuild.googleapis.com    
  gcloud services enable geocoding-backend.googleapis.com  

***???
In the Cloud Console, in the Navigation menu (), click APIs & services > Credentials.
Select Create credentials, select API key from the dropdown menu.
The API key created dialog box displays your newly created key. An API key is a long string containing upper and lower case letters, numbers, and dashes. For example, a4db08b757294ea94c08f2df493465a1.
Click Edit API key in the dialog box.
Select Restrict key in the API restrictions section to add API restrictions for your new API key.
Click in the filter box and type Geocoding API.
Select Geocoding API and click OK.
Click the Save button.
***

Copy the lab source files into your Cloud Shell
The cloud function with predefined code is hosted on a remote Cloud Storage bucket. You will now copy these source files into your Cloud Shell. These files include the source code for the Cloud Function and the schema for the BigQuery table that you will create in the lab.
In Cloud Shell, enter the following command to clone the source repository for the lab:

 mkdir ./document-ai-challenge
  gsutil -m cp -r gs://cloud-training/gsp367/* \
    ~/document-ai-challenge/





###
Task 2. Create a form processor
In this task, you must create an instance of the general form processor using the Document AI Form Parser processor in the General (non-specialized) section. The general form processor will process any type of document and extract all the text content it can identify in the document as well as form information that it infers from the layout.
Create the processor using the following configuration details:
Property	Value
Processor Type	Form Parser
Processor Name	finance-processor
Region		US

Note: You will configure a Cloud Function later in this lab with the PROCESSOR ID and PARSER LOCATION of this processor so that the Cloud Function will use this specific processor to process invoices. Click on the created processor and note the PROCESSOR ID. However, the processor region is the PARSER LOCATION.

***
In the console, navigation menu and select Document AI > Overview.
Click Explore Processor and Click Create Processor for Form Parser.
Specify the processor name as ______ and select the region US (United States) from the list.
Click Create to create your processor.
***






### 
Task 3. Create Google Cloud resources
In this task, you must prepare your environment by creating the Google Cloud Storage and BigQuery resources that are required for your document processing pipeline.
Create input, output, and archive Cloud Storage buckets
In this step, you must create the three Cloud Storage buckets listed below with uniform bucket level access enabled.

Bucket Name		Purpose				Storage class	Location
input_bucket_name	For input invoices		Standard	REGION
output_bucket_name	For storing processed data	Standard	REGION
archive_bucket_name	For archiving invoices		Standard	REGION

***
In Cloud Shell, enter the following command to create the Cloud Storage buckets for the lab:

 export PROJECT_ID=$(gcloud config get-value core/project)
  export BUCKET_LOCATION=us-central1
  gsutil mb -c standard -l ${BUCKET_LOCATION} -b on \
    gs://${PROJECT_ID}-input-invoices
  gsutil mb -c standard -l ${BUCKET_LOCATION} -b on \
    gs://${PROJECT_ID}-output-invoices
  gsutil mb -c standard -l ${BUCKET_LOCATION} -b on \
    gs://${PROJECT_ID}-archived-invoices
***


Note: A bucket can be created using the gsutil tool with the -mb parameter, along with -c parameter to set the storage class, -l to set the (regional) location and the -b flag with the value of on or off to set uniform bucket level access. Read the mb - Make buckets reference for more about creating buckets using gsutil.

Create a BigQuery dataset and tables
In this step, you must create a BigQuery dataset and the output table required for your data processing pipeline.
Dataset
Dataset Name		Location
invoice_parser_results	US

Note: Use bq mk to create BigQuery resources. The command line switch parameter -d is used to create a dataset and --location is used to set the location of the resource. Read the Create datasets guide for more information about creating datasets using the bq command-line tool.

Table
The table schema for the extracted information has been provided for you in the JSON file document-ai-challenge/scripts/table-schema/doc_ai_extracted_entities.json. Use this schema to create a table named doc_ai_extracted_entities in the invoice_parser_results dataset.

Note: Use bq mk to create BigQuery resources. The command line switch --table is used to create a table. For more information about creating tables with a schema definition using the bq command-line tool, read the Create and use tables guide.
You can navigate to BigQuery in the Cloud Console and inspect the schema of tables in the invoice_parser_results dataset using BigQuery SQL workspace.

***
In Cloud Shell, enter the following command to create the BigQuery tables for the lab:


*** 
 bq --location=US mk  -d \
     --description "Form Parser Results" \
     ${PROJECT_ID}:invoice_parser_results
  cd ~/documentai-pipeline-demo/scripts/table-schema/
  bq mk --table \
    invoice_parser_results.doc_ai_extracted_entities \
    doc_ai_extracted_entities.json
  bq mk --table \
    invoice_parser_results.geocode_details \
    geocode_details.json
***




###
Task 4. Deploy the document processing Cloud Function
you must deploy the Cloud Function. This function will use a Document AI API Generic Form processor to extract form data from the raw documents.
You can examine the source code of the Cloud Function using the Code Editor or any other editor of your choice. The Cloud Function is stored in the following folders in Cloud Shell:
Process Invoices - scripts/cloud-functions/process-invoices

The Cloud Function, process-invoices, must be triggered when files are uploaded to the input files storage bucket you created earlier.
Deploy the Cloud Function to process documents uploaded to Cloud Storage
Deploy a Cloud Function that uses a Document AI form processor to parse form documents that have been uploaded to a Cloud Storage bucket.
Navigate to scripts directory:

cd ~/document-ai-challenge/scripts


Deploy the Cloud Function:

 export PROJECT_ID=$(gcloud config get-value core/project)   
  export CLOUD_FUNCTION_LOCATION=us-central1
  gcloud functions deploy process-invoices \
  --region=${CLOUD_FUNCTION_LOCATION} \
  --entry-point=process_invoice \
  --runtime=python37 \
  --service-account=${PROJECT_ID}@appspot.gserviceaccount.com \
  --source=cloud-functions/process-invoices \
  --timeout=400 \
  --env-vars-file=cloud-functions/process-invoices/.env.yaml \
  --trigger-resource=gs://${PROJECT_ID}-input-invoices \
  --trigger-event=google.storage.object.finalize


If you inspect the Cloud Function source code you will see that the function gets the Document AI processor details via two runtime environment variables.
You will have to reconfigure the Cloud Function deployment so that the environment variablesPROCESSOR_ID and PARSER_LOCATION contain the correct values for the Form Parser processor you deployed in a previous step.
Make sure the PARSER_LOCATION value must be in lower case.
Wait for the function to be fully redeployed.




###
Task 5. Test and validate the end-to-end solution
For your final task you must successfully process the set of invoices that are available in the ~/document-ai-challenge/invoices folder using your pipeline.
Upload these invoices to the input Cloud Storage bucket and monitor the progress of the pipeline.
Watch the events until you see a final event indicating that the function execution finished with a status of OK.
Once the pipeline has fully processed the documents, you will see that the form information that is extracted from the invoices by the Document AI processor has been written out into the BigQuery table.

Note: To monitor progress, click Logs in the Management section of the Cloud function to view logs.
Note : You may see some errors that do not affect document processing significantly, especially timeouts, in this lab. If you not see data being reported as being written to BigQuery within 5 minutes in the logs then double check that the parameters set in the .env.yaml file in the previous section are correct and try again.
In particular make sure the Processor ID and location variables that you set are valid and note that the location parameter must be in lower case. Also note that the event list does not automatically refresh.

***
In Cloud Shell, enter the following command to upload sample forms to the Cloud Storage bucket that will trigger the process-invoices Cloud Function:


export PROJECT_ID=$(gcloud config get-value core/project)
  gsutil -m cp -r gs://cloud-training/gsp367/* \
    ~/document-ai-challenge/invoices gs://${PROJECT_ID}-input-invoices/

In the Cloud Console, on the Navigation menu (), click Cloud Functions.
Click the Cloud Function process-invoices to open its management page.
Click Logs.

***
In the Cloud Console, on the Navigation menu (), click BigQuery.
Expand your Project ID in the Explorer.
Expand invoice_parser_results.
Select doc_ai_extracted_entities and click Preview. You will see the form information extracted from the invoices by the invoice processor. You can see that address information and the supplier name has been detected.
Select geocode_details and click Preview. You will see the formatted address, latitude, and longitude for each invoice that has been processed that contained address data that Document AI was able to extract.






Congratulations
Over the course of this challenge lab you have demonstrated your knowledge of working with Document AI.

