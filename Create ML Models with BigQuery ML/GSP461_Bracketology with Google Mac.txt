# Google Cloud Console, select Navigation menu > BigQuery.
# The Welcome message box opens. This message box provides a link to the quickstart guide and the release notes.
# Click Done.


# Task 1. Open the BigQuery console
# open the Navigation menu and select BigQuery.
# Click DONE to take yourself to the beta UI. 


# Task 2. NCAA March Madness

# Task 3. Find the NCAA public dataset in BigQuery
# Make sure that you are still in the BigQuery Console for this step. In the Explorer tab, click the + ADD button then select Public datasets.
# In the search bar, type in NCAA Basketball and hit enter.
# One result will pop up—select it and then click VIEW DATASET
# Expand bigquery-public-data > ncaa_basketball dataset to reveal its tables
# You should see 10 tables in the dataset.
# Click on the mbb_historical_tournament_games and then click PREVIEW to see sample rows of data.
# Then click DETAILS to get metadata about the table.


# Task 4. Write a query to determine available seasons and games
SELECT
  season,
  COUNT(*) as games_per_tournament
  FROM
 `bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`
 GROUP BY season
 ORDER BY season # default is Ascending (low to high)


# Task 5. Understand machine learning features and labels
#  In ML, each column of data that will help us determine the outcome (win or loss for a tournament game) is called a feature.
# The column of data that you are trying to predict is called the label. ML models "learn" the association between features to predict the outcome of a label.


# Task 6. Create a labeled ML dataset
# Building a machine learning model requires a lot of high-quality training data. Fortunately, our NCAA dataset is robust enough where we can rely upon it to build an effective model.
# Return to the BigQuery Console—you should have left off on the result of the query you ran.
# From the left-hand menu, open the mbb_historical_tournament_games table by clicking on the table name. Then once it loads, click PREVIEW

# After inspecting the dataset, you'll notice that one row in the dataset has columns for both win_market and lose_market.
# You need to break the single game record into a record for each team so you can label each row as a "winner" or "loser".

# create a row for the winning team
SELECT
  # features
  season, # ex: 2015 season has March 2016 tournament games
  round, # sweet 16
  days_from_epoch, # how old is the game
  game_date,
  day, # Friday

  'win' AS label, # our label

  win_seed AS seed, # ranking
  win_market AS market,
  win_name AS name,
  win_alias AS alias,
  win_school_ncaa AS school_ncaa,
  # win_pts AS points,

  lose_seed AS opponent_seed, # ranking
  lose_market AS opponent_market,
  lose_name AS opponent_name,
  lose_alias AS opponent_alias,
  lose_school_ncaa AS opponent_school_ncaa
  # lose_pts AS opponent_points

FROM `bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`

UNION ALL

# create a separate row for the losing team
SELECT
# features
  season,
  round,
  days_from_epoch,
  game_date,
  day,

  'loss' AS label, # our label

  lose_seed AS seed, # ranking
  lose_market AS market,
  lose_name AS name,
  lose_alias AS alias,
  lose_school_ncaa AS school_ncaa,
  # lose_pts AS points,

  win_seed AS opponent_seed, # ranking
  win_market AS opponent_market,
  win_name AS opponent_name,
  win_alias AS opponent_alias,
  win_school_ncaa AS opponent_school_ncaa
  # win_pts AS opponent_points

FROM
`bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`



# Task 7. Create a machine learning model to predict the winner based on seed and team name
#  Since we have two classes, win or lose, it's also called a binary classification model. A team can either win or lose a match.
# Our classification model will be doing machine learning with a widely used statistical model called Logistic Regression.


## Creating a machine learning model with BigQuery ML
# In the Explorer tab, click on the View actions icon next to your Project ID and select Create dataset.
# This will open a Create dataset dialog. Set your Dataset ID to bracketology and click CREATE DATASET.
CREATE OR REPLACE MODEL
  `bracketology.ncaa_model`
OPTIONS
  ( model_type='logistic_reg') AS

# create a row for the winning team
SELECT
  # features
  season,

  'win' AS label, # our label

  win_seed AS seed, # ranking
  win_school_ncaa AS school_ncaa,

  lose_seed AS opponent_seed, # ranking
  lose_school_ncaa AS opponent_school_ncaa

FROM `bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`
WHERE season <= 2017

UNION ALL

# create a separate row for the losing team
SELECT
# features
  season,

  'loss' AS label, # our label
  lose_seed AS seed, # ranking
  lose_school_ncaa AS school_ncaa,

  win_seed AS opponent_seed, # ranking
  win_school_ncaa AS opponent_school_ncaa

FROM
`bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`

# now we split our dataset with a WHERE clause so we can train on a subset of data and then evaluate and test the model's performance against a reserved subset so the model doesn't memorize or overfit to the training data.

# tournament season information from 1985 - 2017
# here we'll train on 1985 - 2017 and predict for 2018
WHERE season <= 2017

# View model training stats
# During the training process, the model will optimize a path to it's best possible weighting of each feature.
#  With each run it is trying to minimize  Training Data Loss and Evaluation Data Loss.
# Should you ever find that the final loss for evaluation is much higher than for training, 
# your model is overfitting or memorizing your training data instead of learning generalizable relationships.
# You can view how many training runs the model takes by clicking the TRAINING tab and selecting Table under View as option.


# See what the model learned about our features
SELECT
  category,
  weight
FROM
  UNNEST((
    SELECT
      category_weights
    FROM
      ML.WEIGHTS(MODEL `bracketology.ncaa_model`)
    WHERE
      processed_input = 'seed')) # try other features like 'school_ncaa'
      ORDER BY weight DESC


# Task 8. Evaluate model performance
SELECT
  *
FROM
  ML.EVALUATE(MODEL   `bracketology.ncaa_model`)


### The value will be around 69% accurate. While it's better than a coin flip, there is room for improvement.
# Note: For classification models, model accuracy isn't the only metric in the output you should care about.
# Because you performed a logistic regression, you can evaluate your model performance against all of the following metrics (the closer to 1.0 the better):
# Precision: A metric for classification models. Precision identifies the frequency with which a model was correct when predicting the positive class.
# Recall: A metric for classification models that answers the following question: 
# Out of all the possible positive labels, how many did the model correctly identify?
# Accuracy: Accuracy is the fraction of predictions that a classification model got right.
# f1_score: A measure of the accuracy of the model. The f1 score is the harmonic average of the precision and recall.
# An f1 score's best value is 1. The worst value is 0.
# log_loss: The loss function used in a logistic regression. This is the measure of how far the model's predictions are from the correct labels.
# roc_auc: The area under the ROC curve. This is the probability that a classifier is more confident that a randomly 
# chosen positive example is actually positive than that a randomly chosen negative example is positive.



# Task 9. Making predictions
CREATE OR REPLACE TABLE `bracketology.predictions` AS (

SELECT * FROM ML.PREDICT(MODEL `bracketology.ncaa_model`,

# predicting for 2018 tournament games (2017 season)
(SELECT * FROM `data-to-insights.ncaa.2018_tournament_results`)
)
)

# Task 10. How many did our model get right for the 2018 NCAA tournament?
SELECT * FROM `bracketology.predictions`
WHERE predicted_label <> label

# Out of 134 predictions (67 March tournament games), our model got it wrong 38 times. 70% overall for the 2018 tournament matchup.
(where is it seen?)


# Task 11. Models can only take you so far...
# Let's find biggest upset for the 2017 tournament according to the model. 
# We'll look where the model predicts with 80%+ confidence and gets it WRONG.
SELECT
  model.label AS predicted_label,
  model.prob AS confidence,

  predictions.label AS correct_label,

  game_date,
  round,

  seed,
  school_ncaa,
  points,

  opponent_seed,
  opponent_school_ncaa,
  opponent_points

FROM `bracketology.predictions` AS predictions,
UNNEST(predicted_label_probs) AS model

WHERE model.prob > .8 AND predicted_label <> predictions.label

# Prediction: The model predicts Seed 1 Virginia to beat Seed 16 UMBC with 87% confidence. Seems reasonable right?


# Task 12. Using skillful ML model features
# Create a new ML dataset with these skillful features

# create training dataset:
# create a row for the winning team
CREATE OR REPLACE TABLE `bracketology.training_new_features` AS
WITH outcomes AS (
SELECT
  # features
  season, # 1994

  'win' AS label, # our label

  win_seed AS seed, # ranking # this time without seed even
  win_school_ncaa AS school_ncaa,

  lose_seed AS opponent_seed, # ranking
  lose_school_ncaa AS opponent_school_ncaa

FROM `bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games` t
WHERE season >= 2014

UNION ALL

# create a separate row for the losing team
SELECT
# features
  season, # 1994

  'loss' AS label, # our label

  lose_seed AS seed, # ranking
  lose_school_ncaa AS school_ncaa,

  win_seed AS opponent_seed, # ranking
  win_school_ncaa AS opponent_school_ncaa

FROM
`bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games` t
WHERE season >= 2014

UNION ALL

# add in 2018 tournament game results not part of the public dataset:
SELECT
  season,
  label,
  seed,
  school_ncaa,
  opponent_seed,
  opponent_school_ncaa
FROM
  `data-to-insights.ncaa.2018_tournament_results`

)

SELECT
o.season,
label,

# our team
  seed,
  school_ncaa,
  # new pace metrics (basketball possession)
  team.pace_rank,
  team.poss_40min,
  team.pace_rating,
  # new efficiency metrics (scoring over time)
  team.efficiency_rank,
  team.pts_100poss,
  team.efficiency_rating,

# opposing team
  opponent_seed,
  opponent_school_ncaa,
  # new pace metrics (basketball possession)
  opp.pace_rank AS opp_pace_rank,
  opp.poss_40min AS opp_poss_40min,
  opp.pace_rating AS opp_pace_rating,
  # new efficiency metrics (scoring over time)
  opp.efficiency_rank AS opp_efficiency_rank,
  opp.pts_100poss AS opp_pts_100poss,
  opp.efficiency_rating AS opp_efficiency_rating,

# a little feature engineering (take the difference in stats)

  # new pace metrics (basketball possession)
  opp.pace_rank - team.pace_rank AS pace_rank_diff,
  opp.poss_40min - team.poss_40min AS pace_stat_diff,
  opp.pace_rating - team.pace_rating AS pace_rating_diff,
  # new efficiency metrics (scoring over time)
  opp.efficiency_rank - team.efficiency_rank AS eff_rank_diff,
  opp.pts_100poss - team.pts_100poss AS eff_stat_diff,
  opp.efficiency_rating - team.efficiency_rating AS eff_rating_diff

FROM outcomes AS o
LEFT JOIN `data-to-insights.ncaa.feature_engineering` AS team
ON o.school_ncaa = team.team AND o.season = team.season
LEFT JOIN `data-to-insights.ncaa.feature_engineering` AS opp
ON o.opponent_school_ncaa = opp.team AND o.season = opp.season


# Task 13. Preview the new features
# Click on the Go to table button on the right-hand side of the console. Then click on the Preview tab.

# Task 14. Interpreting selected metrics
# opp_efficiency_rank
# Opponent's Efficiency Rank: out of all the teams, what rank does our opponent have for scoring efficiently over 
# time (points per 100 basketball possessions). Lower is better.

# opp_pace_rank
# Opponent's Pace Rank: out of all teams, what rank does our opponent have for basketball possession (number of possessions in 40 minutes). Lower is better.

# Now that you have insightful features on how well a team can score and how well it can hold on to the basketball lets train our second model.

#As an additional measure to safe-guard your model from "memorizing good teams from the past", 
# exclude the team's name and the seed from this next model and focus only on the metrics.

# Task 15. Train the new model

CREATE OR REPLACE MODEL
  `bracketology.ncaa_model_updated`
OPTIONS
  ( model_type='logistic_reg') AS

SELECT
  # this time, don't train the model on school name or seed
  season,
  label,

  # our pace
  poss_40min,
  pace_rank,
  pace_rating,

  # opponent pace
  opp_poss_40min,
  opp_pace_rank,
  opp_pace_rating,

  # difference in pace
  pace_rank_diff,
  pace_stat_diff,
  pace_rating_diff,


  # our efficiency
  pts_100poss,
  efficiency_rank,
  efficiency_rating,

  # opponent efficiency
  opp_pts_100poss,
  opp_efficiency_rank,
  opp_efficiency_rating,

  # difference in efficiency
  eff_rank_diff,
  eff_stat_diff,
  eff_rating_diff

FROM `bracketology.training_new_features`

# here we'll train on 2014 - 2017 and predict on 2018
WHERE season BETWEEN 2014 AND 2017 # between in SQL is inclusive of end points



# Task 16. Evaluate the new model's performance
SELECT
  *
FROM
  ML.EVALUATE(MODEL     `bracketology.ncaa_model_updated`)
# You just trained a new model with different features and increased your accuracy to around 75% or a 5% lift from the original model.



# Task 17. Inspect what the model learned
SELECT
  *
FROM
  ML.WEIGHTS(MODEL     `bracketology.ncaa_model_updated`)
ORDER BY ABS(weight) DESC

# pace_stat_diff
# How different the actual stat for (possessions / 40 minutes) was between teams. According to the model, this is the largest driver in choosing the game outcome.
# eff_stat_diff
# How different the actual stat for (net points / 100 possessions) was between teams.
# eff_rating_diff
# How different the normalized rating for scoring efficiency was between teams.



# Task 18. Prediction time!
CREATE OR REPLACE TABLE `bracketology.ncaa_2018_predictions` AS

# let's add back our other data columns for context
SELECT
  *
FROM
  ML.PREDICT(MODEL     `bracketology.ncaa_model_updated`, (

SELECT
* # include all columns now (the model has already been trained)
FROM `bracketology.training_new_features`

WHERE season = 2018

))


# Task 19. Prediction analysis:
SELECT * FROM `bracketology.ncaa_2018_predictions`
WHERE predicted_label <> label


# Task 20. Where were the upsets in March 2018?

SELECT
CONCAT(school_ncaa, " was predicted to ",IF(predicted_label="loss","lose","win")," ",CAST(ROUND(p.prob,2)*100 AS STRING), "% but ", IF(n.label="loss","lost","won")) AS narrative,
predicted_label, # what the model thought
n.label, # what actually happened
ROUND(p.prob,2) AS probability,
season,

# us
seed,
school_ncaa,
pace_rank,
efficiency_rank,

# them
opponent_seed,
opponent_school_ncaa,
opp_pace_rank,
opp_efficiency_rank

FROM `bracketology.ncaa_2018_predictions` AS n,
UNNEST(predicted_label_probs) AS p
WHERE
  predicted_label <> n.label # model got it wrong
  AND p.prob > .75  # by more than 75% confidence
ORDER BY prob DESC




# Task 21. Comparing model performance
SELECT
CONCAT(opponent_school_ncaa, " (", opponent_seed, ") was ",CAST(ROUND(ROUND(p.prob,2)*100,2) AS STRING),"% predicted to upset ", school_ncaa, " (", seed, ") and did!") AS narrative,
predicted_label, # what the model thought
n.label, # what actually happened
ROUND(p.prob,2) AS probability,
season,

# us
seed,
school_ncaa,
pace_rank,
efficiency_rank,

# them
opponent_seed,
opponent_school_ncaa,
opp_pace_rank,
opp_efficiency_rank,

(CAST(opponent_seed AS INT64) - CAST(seed AS INT64)) AS seed_diff

FROM `bracketology.ncaa_2018_predictions` AS n,
UNNEST(predicted_label_probs) AS p
WHERE
  predicted_label = 'loss'
  AND predicted_label = n.label # model got it right
  AND p.prob >= .55  # by 55%+ confidence
  AND (CAST(opponent_seed AS INT64) - CAST(seed AS INT64)) > 2 # seed difference magnitude
ORDER BY (CAST(opponent_seed AS INT64) - CAST(seed AS INT64)) DESC



# Task 22. Predicting for the 2019 March Madness tournament
SELECT * FROM `data-to-insights.ncaa.2019_tournament_seeds` WHERE seed = 1

# Run the below query to get all possible team games in the tournament.
SELECT
  NULL AS label,
  team.school_ncaa AS team_school_ncaa,
  team.seed AS team_seed,
  opp.school_ncaa AS opp_school_ncaa,
  opp.seed AS opp_seed
FROM `data-to-insights.ncaa.2019_tournament_seeds` AS team
CROSS JOIN `data-to-insights.ncaa.2019_tournament_seeds` AS opp
# teams cannot play against themselves :)
WHERE team.school_ncaa <> opp.school_ncaa


# Make predictions
CREATE OR REPLACE TABLE `bracketology.ncaa_2019_tournament_predictions` AS

SELECT
  *
FROM
  # let's predicted using the newer model
  ML.PREDICT(MODEL     `bracketology.ncaa_model_updated`, (

# let's predict on March 2019 tournament games:
SELECT * FROM `bracketology.ncaa_2019_tournament`
))


# Get your predictions]
SELECT
  p.label AS prediction,
  ROUND(p.prob,3) AS confidence,
  school_ncaa,
  seed,
  opponent_school_ncaa,
  opponent_seed
FROM `bracketology.ncaa_2019_tournament_predictions`,
UNNEST(predicted_label_probs) AS p
WHERE p.prob >= .5
AND school_ncaa = 'Duke'
ORDER BY seed, opponent_seed



