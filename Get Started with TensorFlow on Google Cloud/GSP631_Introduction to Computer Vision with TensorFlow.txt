# Task 1. Access Cloud Code
# Application Menu -> Terminal -> New Terminal
python --version

python -c "import tensorflow;print(tensorflow.__version__)"

pip3 install --upgrade pip

/usr/bin/python3 -m pip install -U google-cloud-logging --user

/usr/bin/python3 -m pip install -U pylint --user

pip install --upgrade tensorflow




# Task 2. Start Coding
# Application Menu > File > New File

# Task 3. Import necessary packages
# Import and configure logging
import logging
import google.cloud.logging as cloud_logging
from google.cloud.logging.handlers import CloudLoggingHandler
from google.cloud.logging_v2.handlers import setup_logging

cloud_logger = logging.getLogger('cloudLogger')
cloud_logger.setLevel(logging.INFO)
cloud_logger.addHandler(CloudLoggingHandler(cloud_logging.Client()))
cloud_logger.addHandler(logging.StreamHandler())



# Import TensorFlow
import tensorflow as tf

# Import numpy
import numpy as np


# Import tensorflow_datasets
import tensorflow_datasets as tfds


# Task 4. Loading and preprocessing the dataset
### link to dataset https://www.tensorflow.org/datasets/catalog/fashion_mnist

# Define, load and configure data
(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)
## (you set the split argument to specify which splits of the dataset is to be loaded. You set as_supervised to True to ensure that the loaded # tf.data.Dataset will have a 2-tuple structure (input, label)

#  to see the min and max values of training images for item 0.
# Values before normalization
image_batch, labels_batch = next(iter(ds_train))
print("Before normalization ->", np.min(image_batch[0]), np.max(image_batch[0]))

# Define batch size
BATCH_SIZE = 32

# When training a neural network, for various reasons it's easier if you scale the pixel values to the range between 0 to 1. 
#This process is called 'normalization'.

# Normalize and batch process the dataset
ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)
ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)

# Examine the min and max values of the batch after normalization
image_batch, labels_batch = next(iter(ds_train))
print("After normalization ->", np.min(image_batch[0]), np.max(image_batch[0]))



#Task 5. Design your model
# Define the model
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

## 
# Sequential: This defines a SEQUENCE of layers in the neural network.
# Flatten: Our images are of shape (28, 28), i.e, the values are in the form of a square matrix. Flatten takes that square and 
# turns it into a one-dimensional vector.
# Dense: Adds a layer of neurons.
# Relu effectively means if X>0 return X, else return 0. It passes values 0 or greater to the next layer in the network.
# Softmax takes a set of values, and effectively picks the biggest one so you don't have to sort to find the largest value. For example, if the output of # # the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it returns [0,0,0,0,1,0,0,0,0].
# Optimizer is an algorithm that modifies the attributes of the neural network like weights and learning rate.
# Loss indicates the model's performance by a number. 


#Task 6. Compile and train the model
# Compile the model
model.compile(optimizer = tf.keras.optimizers.Adam(),
              loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

model.fit(ds_train, epochs=5)

## Press Ctrl + s or go to Application Menu -> File -> Save. In the prompt, type model.py for name and press OK.

# Run the script
python model.py


# Task 7. Evaluate model performance on unseen data
cloud_logger.info(model.evaluate(ds_test))

# Press Ctrl+s or go toApplication Menu -> File -> Save, to save the changes.
# Run the script:
python model.py



# Task 8. Saving and loading the model
# Save the entire model as a SavedModel.
model.save('saved_model')

# Reload a fresh Keras model from the saved model
new_model = tf.keras.models.load_model('saved_model')

# Summary of loaded SavedModel
new_model.summary()

# Save the entire model to a HDF5 file.
model.save('my_model.h5')

# Recreate the exact same model, including its weights and the optimizer
new_model_h5 = tf.keras.models.load_model('my_model.h5')

# Summary of loaded h5 model
new_model_h5.summary()

# Press Ctrl+s or go toApplication Menu -> File -> Save, to save the changes.
# Run the script:
python model.py



# Task 9. Explore callbacks
# Application Menu -> File -> New File
# Ctrl + s or go toApplication Menu -> File -> Save. In the prompt, type callback_model.py for name and press OK

# Import and configure logging
import logging
import google.cloud.logging as cloud_logging
from google.cloud.logging.handlers import CloudLoggingHandler
from google.cloud.logging_v2.handlers import setup_logging
exp_logger = logging.getLogger('expLogger')
exp_logger.setLevel(logging.INFO)
exp_logger.addHandler(CloudLoggingHandler(cloud_logging.Client(), name="callback"))

# Import tensorflow_datasets
import tensorflow_datasets as tfds
# Import numpy
import numpy as np
# Import TensorFlow
import tensorflow as tf
# Define Callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('sparse_categorical_accuracy')>0.84):
      exp_logger.info("\nReached 84% accuracy so cancelling training!")
      self.model.stop_training = True
callbacks = myCallback()
# Define, load and configure data
(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)
# Define batch size
BATCH_SIZE = 32
# Normalizing and batch processing of data
ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)
ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)
# Define the model
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])
# Compile data
model.compile(optimizer = tf.keras.optimizers.Adam(),
              loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
model.fit(ds_train, epochs=5, callbacks=[callbacks])


# Ctrl + s or go toApplication Menu -> File -> Save, to save the changes.
# Run the script:
python callback_model.py



# Explore
#Exercise 1
# Application Menu -> File -> New File
# Press Ctrl + s or go toApplication Menu -> File -> Save. In the prompt, type updated_model.py for name and press OK.
# Paste the following code into updated_model.py

# Import and configure logging
import logging
import google.cloud.logging as cloud_logging
from google.cloud.logging.handlers import CloudLoggingHandler
from google.cloud.logging_v2.handlers import setup_logging
up_logger = logging.getLogger('upLogger')
up_logger.setLevel(logging.INFO)
up_logger.addHandler(CloudLoggingHandler(cloud_logging.Client(), name="updated"))

# Import tensorflow_datasets
import tensorflow_datasets as tfds
# Import numpy
import numpy as np
# Import TensorFlow
import tensorflow as tf

# Define, load and configure data
(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)
# Define batch size
BATCH_SIZE = 32
# Normalizing and batch processing of data
ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)
ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)
# Define the model
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])
# Compile data
model.compile(optimizer = tf.keras.optimizers.Adam(),
              loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
model.fit(ds_train, epochs=5)
# Logs model summary
model.summary(print_fn=up_logger.info)


## Go to # Define the model section, change 64 to 128 neurons:
# Define the model
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])


# Press Ctrl + s or go toApplication Menu -> File -> Save, to save the changes.
# Run the script:
python updated_model.py



# Exercise 2
### In updated_model.py, add a layer in the # Define the model section.
# Replace your model definition with the following:
# Define the model
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# Press Ctrl + s or go toApplication Menu -> File -> Save, to save the changes.
#Run the script:
python updated_model.py


# Exercise 3
# Give it a try -- in the # Normalize the data section, remove the map function applied to both the training and test datasets
# Define batch size
BATCH_SIZE = 32


# Normalizing and batch processing of data
ds_train = ds_train.batch(BATCH_SIZE)
ds_test = ds_test.batch(BATCH_SIZE)

# Define the model
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])



# Add this code at the end of updated_model.py to print the max value of the first image in batch 0.
# Print out max value to see the changes
image_batch, labels_batch = next(iter(ds_train))
t_image_batch, t_labels_batch = next(iter(ds_test))
up_logger.info("training images max " + str(np.max(image_batch[0])))
up_logger.info("test images max " + str(np.max(t_image_batch[0])))



# Press Ctrl + s or go toApplication Menu -> File -> Save, to save the changes.
#Run the script:
python updated_model.py



## Exercise 4
# if you remove the Flatten() layer?
# In the # Define the model section, remove "tf.keras.layers.Flatten()":
# Define the model
model = tf.keras.models.Sequential([tf.keras.layers.Dense(64, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

#Save and run updated_model.py.


# Exercise 5
#  Why are there 10 neurons in the final layer? What happens if you have a different number than 10?
# Find out by training the network with 5.
# Replace the # Define the model section with the following to undo the change you made in the previous section:
# Define the model
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

# Change the number of neurons 
# Define the model
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(5, activation=tf.nn.softmax)])


# Save and run updated_model.py

# rule of thumb -- the number of neurons in the last layer should match the number of classes you are classifying for. In this case, it's the digits 0-9, so there are 10 of them, and hence you should have 10 neurons in your final layer